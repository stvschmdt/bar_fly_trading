════════════════════════════════════════════════════════════════════════════════
  CROSS-ENCODER MODEL IMPROVEMENT PROPOSALS
  Based on architecture audit of stockformer + 7-model performance analysis
════════════════════════════════════════════════════════════════════════════════


CURRENT STATE SUMMARY
─────────────────────
  reg_3d / reg_10d:  Correlation ~0.025, predict ALL positive (never short)
  bin_3d:            Always predicts "up" (recall=100%, precision=54.5%)
  bin_10d:           Slightly better, but still biased heavily to "up"
  bin_30d:           BEST model — 66.4% precision, 73.5% recall, makes real decisions
  buck_3d:           WORSE than random (7.6% vs 12.5%) — collapsed to 2 buckets
  reg_30d:           Only regression model with meaningful correlation (0.14)

  Verdict: Short-horizon models are essentially broken. 30d models work because
  the signal-to-noise ratio is higher over longer periods.


════════════════════════════════════════════════════════════════════════════════
  CRITICAL FIXES (do these first — biggest impact)
════════════════════════════════════════════════════════════════════════════════

1. ADD POSITIONAL ENCODING
   ─────────────────────────
   The transformer has NO positional encoding. It cannot distinguish day 1
   from day 5 in the lookback window. This is arguably the single biggest
   architectural gap.

   Fix in model.py:
     class PositionalEncoding(nn.Module):
         def __init__(self, d_model, max_len=100):
             super().__init__()
             pe = torch.zeros(max_len, d_model)
             position = torch.arange(0, max_len).unsqueeze(1).float()
             div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                                  -(math.log(10000.0) / d_model))
             pe[:, 0::2] = torch.sin(position * div_term)
             pe[:, 1::2] = torch.cos(position * div_term)
             self.register_buffer('pe', pe.unsqueeze(0))

         def forward(self, x):
             return x + self.pe[:, :x.size(1)]

   Apply after input_proj, before the TransformerEncoder.

   Expected impact: HIGH — enables the model to learn temporal patterns
   (e.g., "rising RSI over last 3 days" vs "falling RSI over last 3 days")


2. EXPAND FEATURE SET (8 → ~30 features)
   ──────────────────────────────────────
   Currently the model only sees: close, volume, 3 ROCs, 2 vol_means, 1 vol.
   The data has 90+ columns including RSI, MACD, CCI, ADX, BBands, PE,
   beta, analyst ratings — but NONE are fed to the model.

   Fix in config.py BASE_FEATURE_COLUMNS, add at minimum:
     - rsi_14, macd, macd_9_ema, cci_14, adx_14, atr_14
     - bbands_upper_20, bbands_middle_20, bbands_lower_20
     - sma_20, sma_50, sma_200, ema_20, ema_50
     - pe_ratio, beta, forward_pe
     - 52_week_high_pct, 52_week_low_pct
     - bull_bear_delta (our screener composite signal)

   Expected impact: VERY HIGH — the model currently can't see any of the
   technical indicators it's supposed to be learning from. This is like
   asking someone to predict the market using only price and volume.


3. FIX TRAIN/VAL SPLIT — USE TEMPORAL SPLIT, NOT RANDOM
   ──────────────────────────────────────────────────────
   Current: random 80/20 shuffle → future data leaks into training
   Fix: split by date (train up to cutoff, validate after)

   In dataset.py, replace random split with:
     cutoff = sorted_dates[int(len(sorted_dates) * 0.8)]
     train_mask = dates <= cutoff
     val_mask = dates > cutoff

   Expected impact: HIGH — current validation metrics are artificially
   inflated by look-ahead bias. True OOS performance is likely worse.


4. FIX NORMALIZATION LEAKAGE
   ──────────────────────────
   Z-score is computed on train+val combined before splitting.
   Fix: compute mean/std on training data only, apply to both.
   Also: save mean/std to disk for consistent inference.

   Expected impact: MEDIUM — affects all metrics, especially short horizons


════════════════════════════════════════════════════════════════════════════════
  ARCHITECTURE IMPROVEMENTS (high-value, moderate effort)
════════════════════════════════════════════════════════════════════════════════

5. REPLACE SINGLE LINEAR HEAD WITH 2-LAYER MLP
   ─────────────────────────────────────────────
   Current: Linear(128, output_dim) — too simple for final prediction
   Fix:
     nn.Sequential(
         nn.Linear(d_model, d_model // 2),
         nn.GELU(),
         nn.Dropout(0.2),
         nn.Linear(d_model // 2, output_dim)
     )

   Expected impact: MEDIUM-HIGH — allows nonlinear transformation before
   prediction. Especially helps regression models learn complex mappings.


6. ADD ATTENTION POOLING (replace last-step pooling)
   ──────────────────────────────────────────────────
   Current: takes only enc[:, -1, :] — discards 80% of encoded info
   Fix: learned attention pooling or mean pooling

     class AttentionPooling(nn.Module):
         def __init__(self, d_model):
             super().__init__()
             self.attn = nn.Linear(d_model, 1)

         def forward(self, x):
             weights = F.softmax(self.attn(x).squeeze(-1), dim=1)
             return (x * weights.unsqueeze(-1)).sum(dim=1)

   Expected impact: MEDIUM — captures patterns across the full window


7. INCREASE LOOKBACK FROM 5 → 20-30
   ──────────────────────────────────
   5 days is extremely short. Most technicals need 14-20 days of context
   (RSI uses 14 periods, MACD needs 26, BBands needs 20).

   With positional encoding added, the model can handle longer sequences.
   Try lookback=20 first, then 30.

   Expected impact: MEDIUM-HIGH — more context = better pattern recognition


8. ADD LEARNING RATE SCHEDULER + WARMUP
   ─────────────────────────────────────
   Current: constant lr=0.001 for all epochs
   Fix: linear warmup (3 epochs) + cosine annealing

     from torch.optim.lr_scheduler import CosineAnnealingLR
     scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)

     # Optional warmup wrapper:
     warmup_epochs = 3
     for epoch in range(num_epochs):
         if epoch < warmup_epochs:
             for pg in optimizer.param_groups:
                 pg['lr'] = cfg['lr'] * (epoch + 1) / warmup_epochs
         ...
         scheduler.step()

   Expected impact: MEDIUM — prevents early divergence, allows finer
   convergence. Especially important with more features.


9. ADD GRADIENT CLIPPING
   ──────────────────────
   Transformers are prone to gradient spikes. Simple fix:

     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

   Add right before optimizer.step() in the training loop.

   Expected impact: LOW-MEDIUM — prevents rare training instabilities


10. USE FOCAL LOSS FOR CLASSIFICATION (already implemented, just unused)
    ────────────────────────────────────────────────────────────────────
    The codebase has FocalLoss(gamma=2.0) in losses.py but it's never used.
    For bucket classification with heavy class imbalance (extreme returns
    are rare), focal loss down-weights easy examples.

    Fix: add --loss-name CLI arg, default to 'focal' for buckets mode.
    Also add class weights based on inverse frequency.

    Expected impact: HIGH for bucket model (currently worse than random)


════════════════════════════════════════════════════════════════════════════════
  TRAINING IMPROVEMENTS (easy wins)
════════════════════════════════════════════════════════════════════════════════

11. EARLY STOPPING + BEST MODEL CHECKPOINT
    ───────────────────────────────────────
    Current: always saves final epoch model
    Fix: track best validation loss, save that model

      best_val_loss = float('inf')
      patience = 5
      no_improve = 0
      for epoch in range(...):
          ...
          if val_loss < best_val_loss:
              best_val_loss = val_loss
              torch.save(model.state_dict(), best_path)
              no_improve = 0
          else:
              no_improve += 1
              if no_improve >= patience:
                  break

    Expected impact: MEDIUM — prevents overfitting, saves best model


12. INCREASE EPOCHS TO 50-100 (with early stopping)
    ────────────────────────────────────────────────
    15 epochs is very few. With early stopping as safety net, run longer.
    bin_30d and reg_30d were still improving at 15 — need more time.

    Expected impact: MEDIUM — already saw improvement from 15→30 epochs


13. SWITCH TO ADAMW WITH EXPLICIT WEIGHT DECAY
    ────────────────────────────────────────────
    Current: Adam with no weight decay → no regularization
    Fix: AdamW with weight_decay=0.01

    Expected impact: LOW-MEDIUM — prevents parameter blow-up


14. ADD LABEL SMOOTHING FOR BINARY/BUCKET
    ──────────────────────────────────────
    Already in losses.py, just unused. CrossEntropyLoss(label_smoothing=0.1)
    prevents overconfident predictions and improves calibration.

    Expected impact: LOW-MEDIUM — helps probability calibration


════════════════════════════════════════════════════════════════════════════════
  ADVANCED IMPROVEMENTS (higher effort, higher ceiling)
════════════════════════════════════════════════════════════════════════════════

15. IMPLEMENT TRUE CROSS-ATTENTION
    ──────────────────────────────
    Current: market/sector embeddings concatenated as flat features
    Better: separate market encoder + stock decoder with cross-attention

      Stock encoder: processes per-stock technical features
      Market encoder: processes market-level features (SPY, VIX, yields)
      Cross-attention: stock queries attend to market keys/values

    This lets the model learn "how does this stock respond to market regime?"
    rather than just seeing flattened features.


16. MULTI-TASK LEARNING
    ────────────────────
    Train one model that jointly predicts all 3 tasks:
      loss = α * regression_loss + β * binary_loss + γ * bucket_loss

    Shared encoder learns richer representations. Each head specializes.
    The regression head's gradients help the classification heads and vice versa.


17. TEMPORAL CONVOLUTIONAL PRE-ENCODER
    ──────────────────────────────────
    Add 1D convolutions before the transformer to capture local patterns:
      Conv1d(feature_dim, d_model, kernel_size=3, padding=1)
      → TransformerEncoder

    This gives the transformer pre-extracted local features to work with.


18. NaN HANDLING: FORWARD-FILL INSTEAD OF ZERO-FILL
    ─────────────────────────────────────────────────
    Current: fills NaN with 0.0 (a meaningful value for returns)
    Fix: forward-fill within each ticker group, then 0-fill only at start

      df.groupby('ticker')[features].ffill().fillna(0)


════════════════════════════════════════════════════════════════════════════════
  PRIORITY IMPLEMENTATION ORDER
════════════════════════════════════════════════════════════════════════════════

  Phase 1 (this week — biggest bang for buck):
    [1] Add positional encoding to model.py
    [2] Expand feature set to include technicals
    [3] Fix temporal train/val split
    [4] Add gradient clipping
    [9] Add gradient clipping
    Expected: 3d models go from broken → functional

  Phase 2 (next week):
    [5] 2-layer MLP head
    [6] Attention pooling
    [7] Increase lookback to 20
    [8] LR scheduler + warmup
    Expected: all models improve 5-15% across the board

  Phase 3 (following week):
    [10] Focal loss for buckets
    [11] Early stopping + best checkpoint
    [12] 50+ epochs
    [13] AdamW with weight decay
    Expected: bucket models become usable, regression correlations > 0.2

  Phase 4 (architecture upgrade):
    [15] True cross-attention
    [16] Multi-task learning
    [17] Temporal conv pre-encoder
    Expected: step-change improvement, new strategy possibilities


════════════════════════════════════════════════════════════════════════════════
  WHY THE SHORT-HORIZON MODELS FAIL
════════════════════════════════════════════════════════════════════════════════

  The 3-day models collapse to predicting the majority class because:

  1. Signal-to-noise ratio is extremely low at 3 days (daily vol ~2%,
     3-day expected move ~1%). The model can't distinguish signal from noise
     with only 8 features and 5 days of lookback.

  2. With random train/val split + no positional encoding, the model
     literally cannot learn temporal patterns. It's seeing shuffled,
     position-agnostic snapshots.

  3. The output head (single linear layer) is too simple to map the
     128-dim encoder output to a meaningful 3-day return prediction.

  The 30-day models work better because:
  - 30-day returns have 3-5x higher signal-to-noise
  - Trends are more persistent over 30 days
  - Even without temporal information, cross-sectional features
    (low PE, strong momentum, bullish delta) predict 30-day returns

  Fixing items [1]-[3] alone should make the 3-day models functional.


════════════════════════════════════════════════════════════════════════════════
  END
════════════════════════════════════════════════════════════════════════════════
